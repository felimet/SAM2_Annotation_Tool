{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20e0674c",
   "metadata": {},
   "source": [
    "## 設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d516bb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import os\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "import numpy as np\n",
    "\n",
    "# select the device for computation\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"using device: {device}\")\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    # use bfloat16 for the entire notebook\n",
    "    torch.autocast(\"cuda\", dtype=torch.bfloat16).__enter__()\n",
    "    # turn on tfloat32 for Ampere GPUs (https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices)\n",
    "    if torch.cuda.get_device_properties(0).major >= 8:\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "elif device.type == \"mps\":\n",
    "    print(\n",
    "        \"\\nSupport for MPS devices is preliminary. SAM 2 is trained with CUDA and might \"\n",
    "        \"give numerically different outputs and sometimes degraded performance on MPS. \"\n",
    "        \"See e.g. https://github.com/pytorch/pytorch/issues/84936 for a discussion.\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034611e5",
   "metadata": {},
   "source": [
    "## 函數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33fa213",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_mask(mask, ax, obj_id=None, random_color=False):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "    else:\n",
    "        cmap = plt.get_cmap(\"tab10\")\n",
    "        cmap_idx = 0 if obj_id is None else obj_id\n",
    "        color = np.array([*cmap(cmap_idx)[:3], 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    ax.imshow(mask_image)\n",
    "\n",
    "\n",
    "def show_points(coords, labels, ax, marker_size=200):\n",
    "    pos_points = coords[labels==1]\n",
    "    neg_points = coords[labels==0]\n",
    "    ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
    "    ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
    "\n",
    "\n",
    "def show_box(box, ax):\n",
    "    x0, y0 = box[0], box[1]\n",
    "    w, h = box[2] - box[0], box[3] - box[1]\n",
    "    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0, 0, 0, 0), lw=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3d08a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_id_mask(out_mask_logits, ann_frame_dir, ann_frame_idx, show_visualization=False):\n",
    "    \"\"\"\n",
    "    將遮罩轉換為物件ID圖像並儲存\n",
    "    \n",
    "    參數:\n",
    "    out_mask_logits: 模型輸出的遮罩邏輯值\n",
    "    ann_frame_dir: 目錄路徑\n",
    "    ann_frame_idx: 當前處理的幀索引\n",
    "    show_visualization: 是否顯示視覺化結果 (預設: False)\n",
    "    \n",
    "    返回:\n",
    "    id_mask: 生成的ID遮罩陣列\n",
    "    mask_path: 儲存的遮罩檔案路徑\n",
    "    \"\"\"\n",
    "    \n",
    "    # 將輸出轉為布林遮罩並轉換為numpy陣列\n",
    "    masks_np = [(mask > 0.0).cpu().numpy() for mask in out_mask_logits]\n",
    "    \n",
    "    # 獲取遮罩的高度和寬度（參考 show_mask 函數的處理方式）\n",
    "    h, w = masks_np[0].shape[-2:]\n",
    "    # print(f\"遮罩形狀: 高={h}, 寬={w}\")\n",
    "    \n",
    "    # 創建一個空的ID遮罩，背景為0\n",
    "    id_mask = np.zeros((h, w), dtype=np.uint8)\n",
    "    \n",
    "    # 用物件ID值填充每個遮罩對應的區域\n",
    "    for mask_idx, mask in enumerate(masks_np):\n",
    "        # 確保遮罩形狀正確，參考 show_mask 函數中的 reshape 方式\n",
    "        binary_mask = mask.reshape(h, w)\n",
    "        \n",
    "        # 將二值遮罩區域設定為物件ID值 (mask_idx + 1 避免與背景值0混淆)\n",
    "        object_id = mask_idx + 1\n",
    "        id_mask[binary_mask] = object_id\n",
    "    \n",
    "    # 確保儲存目錄存在\n",
    "    save_dir = os.path.join(ann_frame_dir, \"id_masks\")\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # 儲存物件ID遮罩\n",
    "    mask_image = Image.fromarray(id_mask)\n",
    "    mask_path = os.path.join(save_dir, f\"{ann_frame_idx:06d}.png\")\n",
    "    mask_image.save(mask_path)\n",
    "    # print(f\"已儲存ID遮罩至 {mask_path}\")\n",
    "    # print(f\"遮罩中的物件ID值: {np.unique(id_mask)}\")\n",
    "    \n",
    "    # 可視化ID遮罩 (如果需要)\n",
    "    if show_visualization:\n",
    "        plt.figure(figsize=(9, 6))\n",
    "        plt.title(f\"Object ID Mask (frame {ann_frame_idx})\")\n",
    "        plt.imshow(id_mask, cmap='jet')  \n",
    "        plt.colorbar(label='Object ID')\n",
    "        plt.show()\n",
    "    \n",
    "    return id_mask, mask_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c18156",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd D:/Anycode/label-sam2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90473e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sam2.build_sam import build_sam2_video_predictor\n",
    "\n",
    "checkpoint = \"./checkpoints/sam2.1_hiera_large.pt\"\n",
    "model_cfg = \"configs/sam2.1/sam2.1_hiera_l.yaml\"\n",
    "predictor = build_sam2_video_predictor(model_cfg, checkpoint, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4bdc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_dir = \"D:/AnyCode/dataset/videos_frame/20250313_170517-170544_clip_00-08_to_00-15\"\n",
    "\n",
    "# scan all the JPEG frame names in this directory\n",
    "frame_names = [\n",
    "    p for p in os.listdir(video_dir)\n",
    "    if os.path.splitext(p)[-1] in [\".jpg\", \".jpeg\", \".JPG\", \".JPEG\"]\n",
    "]\n",
    "frame_names.sort(key=lambda p: int(os.path.splitext(p)[0]))\n",
    "\n",
    "inference_state = predictor.init_state(video_path=video_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c65cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 如果您使用此“ inference_state”運行任何以前的跟踪，請首先通過`reset_state`重置它。\n",
    "predictor.reset_state(inference_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7ffa8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "frame_idx = 0\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.title(f\"frame {frame_idx}\")\n",
    "plt.imshow(Image.open(os.path.join(video_dir, frame_names[frame_idx])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23039389",
   "metadata": {},
   "source": [
    "## 標記"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86a658d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = {}  # hold all the clicks we add for visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de7d474",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_frame_idx = 170  # the frame index we interact with\n",
    "ann_obj_id = 2  # give a unique id to each object we interact with (it can be any integers)\n",
    "\n",
    "# Let's add a positive click at (x, y) = (200, 300) to get started on the first object\n",
    "points = np.array([[1461, 372], [1484, 493], [1451, 551], [1596, 492], [1463, 500]], dtype=np.float32)\n",
    "# for labels, `1` means positive click and `0` means negative click\n",
    "labels = np.array([1, 0, 1, 1, 1], np.int32)\n",
    "prompts[ann_obj_id] = points, labels\n",
    "_, out_obj_ids, out_mask_logits = predictor.add_new_points_or_box(\n",
    "    inference_state=inference_state,\n",
    "    frame_idx=ann_frame_idx,\n",
    "    obj_id=ann_obj_id,\n",
    "    points=points,\n",
    "    labels=labels,\n",
    ")\n",
    "\n",
    "# show the results on the current (interacted) frame\n",
    "plt.figure(figsize=(16, 9))\n",
    "plt.title(f\"frame {ann_frame_idx}\")\n",
    "plt.imshow(Image.open(os.path.join(video_dir, frame_names[ann_frame_idx])))\n",
    "show_points(points, labels, plt.gca())\n",
    "for i, out_obj_id in enumerate(out_obj_ids):\n",
    "    show_points(*prompts[out_obj_id], plt.gca())\n",
    "    show_mask((out_mask_logits[i] > 0.0).cpu().numpy(), plt.gca(), obj_id=out_obj_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6deb17fd",
   "metadata": {},
   "source": [
    "## 預測\n",
    "包含反向預測、儲存物件ID mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776000ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run propagation throughout the video and collect the results in a dict\n",
    "video_segments = {}  # video_segments contains the per-frame segmentation results\n",
    "id_mask_path = os.path.join('D:/AnyCode/dataset', 'video_annotations')\n",
    "for out_frame_idx, out_obj_ids, out_mask_logits in predictor.propagate_in_video(inference_state, reverse=True):\n",
    "    video_segments[out_frame_idx] = {\n",
    "        out_obj_id: (out_mask_logits[i] > 0.0).cpu().numpy()\n",
    "        for i, out_obj_id in enumerate(out_obj_ids)\n",
    "    }\n",
    "    id_mask, mask_path = save_id_mask(\n",
    "            out_mask_logits=out_mask_logits,\n",
    "            ann_frame_dir=id_mask_path,\n",
    "            ann_frame_idx=out_frame_idx,\n",
    "            show_visualization=False\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2218a762",
   "metadata": {},
   "source": [
    "## 結果呈現"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fecd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# render the segmentation results every few frames\n",
    "# vis_frame_stride = 30\n",
    "# plt.close(\"all\")\n",
    "# for out_frame_idx in range(0, len(frame_names), vis_frame_stride):\n",
    "#     plt.figure(figsize=(6, 4))\n",
    "#     plt.title(f\"frame {out_frame_idx}\")\n",
    "#     plt.imshow(Image.open(os.path.join(video_dir, frame_names[out_frame_idx])))\n",
    "#     for out_obj_id, out_mask in video_segments[out_frame_idx].items():\n",
    "#         show_mask(out_mask, plt.gca(), obj_id=out_obj_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0400a8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# id_mask_path = os.path.join('D:/AnyCode/dataset', 'video_annotations')\n",
    "# id_mask, mask_path = save_id_mask(\n",
    "#     out_mask_logits=out_mask_logits,\n",
    "#     ann_frame_dir=id_mask_path,\n",
    "#     ann_frame_idx=ann_frame_idx,\n",
    "#     show_visualization=False  # 設為True可視覺化結果\n",
    "# )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sam2_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
